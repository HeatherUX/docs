I'll rate the complexity of the text based on sentence structure.

Analyzing the text:
```

AI: ---
title: Sentence Structure Evaluator Accuracy and Comparison
description: Performance of the Sentence Structure Evaluator with baseline and human annotation comparisons
---

import Image from 'next/image'

| **Sentence structure accuracy**  
The current configuration of this evaluator achieves **82% accuracy** against our validation dataset.  
|  
| [Baseline AI evaluator comparison](#baseline-ai-evaluator-comparison)  
[Sentence structure evaluator vs human annotation](#sentence-structure-evaluator-vs-human-annotation)  
[Baseline prompt](#baseline-prompt)  
|

---

We tested more than 500 combinations of three main variables:

- System & user prompts
- Models (including several from OpenAI, Google Gemini, Anthropic, and Meta)
- Temperature settings

This evaluator is **over 99% accurate within one complexity level**.  
For example, it is highly unlikely for a *Slightly Complex* text to be rated as *Very Complex*, or a *Moderately Complex* text to be rated as *Exceedingly Complex*.

---

## Baseline AI evaluator comparison {#baseline-ai-evaluator-comparison}

We tested this evaluator against a baseline using a [baseline prompt](#baseline-prompt) that categorized sentence complexity with the rubric, **without being instructed to categorize sentences into types**.  
The baseline's accuracy varied depending on model and temperature but averaged **~45%** across all configurations tested.

### Accuracy

<Image src="./image2.png" alt="Bar chart showing baseline accuracy at 45% and evaluator accuracy at 82%" />

The Sentence Structure evaluator is **over 80% more accurate** than the naive baseline.

---

## Sentence structure evaluator vs human annotation {#sentence-structure-evaluator-vs-human-annotation}

<Image src="./image3.png" alt="Chart comparing evaluator results with human annotation" />

This chart shows accuracy by complexity level and common error patterns.  
Example: In our validation dataset, **17 sentences were labeled Moderately Complex by annotators**. The evaluator labeled them as:
- 1 Slightly Complex
- 12 Moderately Complex
- 4 Very Complex
- 0 Exceedingly Complex

> **Note:** Our dataset doesn't include *Exceedingly Complex* sentences, as these are rare in grades 2–6. The evaluator may be less reliable with highly intricate academic texts.

---

## Baseline prompt {#baseline-prompt}

We used this prompt to simulate a baseline for comparison — a straightforward implementation an EdTech organization might try when aligning with the SCASS rubric.

#### System prompt
```text
You are an expert in grammar and literacy.
```

#### User prompt
```text
Your job is to rate the complexity of a text with respect to its sentence structure given a complexity rubric.

In your answer, respond with just the complexity label ("Exceedingly Complex", "Very Complex", "Moderately Complex", or "Slightly Complex").

Here is the rubric:
[BEGIN RUBRIC]
- "Exceedingly Complex": Mainly complex sentences with several subordinate clauses or phrases and transition words; sentences often contain multiple concepts
- "Very Complex": Many complex sentences with several subordinate phrases or clauses and transition words
- "Moderately Complex": Primarily simple and compound sentences, with some complex constructions
- "Slightly Complex": Mainly simple sentences
[END RUBRIC]

Here is the text:
[BEGIN TEXT]
{text}
[END TEXT]

{format_instructions}
```